# Data-Science-Lab-Practice

```markdown
# Data Science Labs Overview

This repository contains my solutions and learnings from Data Science Labs 1–6. Each lab focused on a core set of methods and tools that form the foundation of modern data science and machine learning workflows. Below is a concise summary of what I covered in each lab.

---

## Lab 1: Probability, Python Basics & Data Exploration

**Key Skills & Topics**  
- Reviewed fundamental probability results (e.g., Central Limit Theorem)  
- Applied core Python constructs to statistical concepts  
- Loaded, cleaned, and explored tabular data using **Pandas** and **NumPy**

---

## Lab 2: Covariance Analysis & Data Skewness

**Key Skills & Topics**  
- Computed and interpreted **covariance matrices** for multivariate data  
- Learned to **skew** and **unskew** data distributions programmatically  
- Reinforced probability tools in Python for controlling data shape

---

## Lab 3: Regularization & Ensembling

**Key Skills & Topics**  
- Implemented **Ridge** and **Lasso** regression for ℓ₂/ℓ₁ regularization  
- Ran my first **Kaggle** workflow: data import, submission, evaluation  
- Experimented with simple **ensembling** and **stacking** techniques to boost model accuracy

---

## Lab 4: Sparse Modeling & Model Selection

**Key Skills & Topics**  
- Applied sparse regression methods (Lasso, forward selection)  
- Used **cross‑validation** to tune hyperparameters  
- Explored pointers to advanced topics for further study (e.g., high‑dimensional feature selection)

---

## Lab 5: Ensemble Methods & Tree‑Based Models

**Key Skills & Topics**  
- Built and tuned **Random Forest**, **Gradient Boosting**, **XGBoost**, and **CatBoost** models  
- Compared ensemble libraries and dissected their key hyperparameters  
- Prepared for an in‑class **Kaggle competition**, focusing on **AUC** as the primary metric

---

## Lab 6: Neural Network Fundamentals with PyTorch

**Key Skills & Topics**  
- Created **PyTorch** `Dataset` and `DataLoader` objects for custom data pipelines  
- Defined and trained single‑layer and multi‑layer fully connected networks  
- Incorporated non‑linear activation functions (ReLU, sigmoid, etc.)  
- Examined how model **capacity** and random seed affect learning (e.g., XOR problem)

---

<details>
<summary>How to run</summary>

1. **Clone** this repo  
   ```bash
   git clone https://github.com/your‑username/Neural‑Engineering‑Project.git
   cd Neural‑Engineering‑Project
   ```

2. **Install** dependencies  
   ```bash
   pip install -r requirements.txt
   ```

3. **Explore** each lab folder (e.g., `Lab1/`, `Lab2/`, …) for notebooks and data files.

4. **Run** the provided Jupyter notebooks or Python scripts to reproduce analyses.

</details>
```
